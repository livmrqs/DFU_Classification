{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T21:42:26.213491Z",
     "start_time": "2025-09-03T21:42:25.620188Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models import ResNet50_Weights, EfficientNet_B0_Weights\n",
    "from sklearn.model_selection import StratifiedGroupKFold, train_test_split\n",
    "from sklearn.metrics import (roc_auc_score, average_precision_score,\n",
    "                             f1_score, accuracy_score, precision_score,\n",
    "                             recall_score, confusion_matrix)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "\n",
    "from codecarbon import track_emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "812071388a76e153",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T21:43:14.434778Z",
     "start_time": "2025-09-03T21:43:14.418409Z"
    }
   },
   "outputs": [],
   "source": [
    "# Configurações iniciais\n",
    "TASK = \"Ischaemia\"  # \"Infection\" ou \"Ischaemia\"\n",
    "IMG_SIZE = 256\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "MODEL_NAME = \"resnet50\"  # \"efficientnet\" ou \"resnet50\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9eb67776d08aa57e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T21:43:24.119219Z",
     "start_time": "2025-09-03T21:43:24.107146Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Pré-processamento de Imagens\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aa084d0befb7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T21:49:46.981640Z",
     "start_time": "2025-09-03T21:49:46.973385Z"
    }
   },
   "outputs": [],
   "source": [
    "root_dir = \"../data/ischaemia\"\n",
    "\n",
    "\n",
    "# 2. Dataset Customizado\n",
    "class DFUDataset(Dataset):\n",
    "    def __init__(self, root_dir, task=TASK, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.task = task\n",
    "\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.identifiers = []  # Novo campo para identificadores únicos\n",
    "\n",
    "        # Validate that root_dir is a valid path\n",
    "        if not isinstance(root_dir, str):\n",
    "            raise TypeError(f\"Expected root_dir to be a string, but got {type(root_dir).__name__}\")\n",
    "\n",
    "        print(\"TASK: \" + TASK)\n",
    "\n",
    "        positive_dir = os.path.join(root_dir, \"Aug-Positive\")\n",
    "        negative_dir = os.path.join(root_dir, \"Aug-Negative\")\n",
    "\n",
    "\n",
    "        # Carregar imagens positivas\n",
    "        for img_name in os.listdir(positive_dir):\n",
    "            self.image_paths.append(os.path.join(positive_dir, img_name))\n",
    "            self.labels.append(1)\n",
    "            # Extrair identificador único (ex: \"imagem\" de \"imagem_1X_M.jpg\")\n",
    "            identifier = img_name.split('_')[0]\n",
    "            self.identifiers.append(identifier)\n",
    "\n",
    "        # Carregar imagens negativas\n",
    "        for img_name in os.listdir(negative_dir):\n",
    "            self.image_paths.append(os.path.join(negative_dir, img_name))\n",
    "            self.labels.append(0)\n",
    "            identifier = img_name.split('_')[0]\n",
    "            self.identifiers.append(identifier)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4ccffabc9c25d33f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T21:49:49.385939Z",
     "start_time": "2025-09-03T21:49:49.380367Z"
    }
   },
   "outputs": [],
   "source": [
    "# 3. Modelo com Regularização\n",
    "def create_model():\n",
    "    if MODEL_NAME == \"resnet50\":\n",
    "        weights = ResNet50_Weights.IMAGENET1K_V1  # Ou ResNet50_Weights.DEFAULT\n",
    "        model = models.resnet50(weights=weights)\n",
    "        num_ftrs = model.fc.in_features\n",
    "        model.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(num_ftrs, 1)\n",
    "        )\n",
    "    elif MODEL_NAME == \"efficientnet\":\n",
    "        weights = EfficientNet_B0_Weights.IMAGENET1K_V1  # Ou EfficientNet_B0_Weights.DEFAULT\n",
    "        model = models.efficientnet_b0(weights=weights)\n",
    "        num_ftrs = model.classifier[1].in_features\n",
    "        model.classifier[1] = nn.Sequential(\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(num_ftrs, 1)\n",
    "        )\n",
    "    return model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "57bf69a01c12e55b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T21:49:50.920492Z",
     "start_time": "2025-09-03T21:49:50.916065Z"
    }
   },
   "outputs": [],
   "source": [
    "# 6. Cálculo de Métricas\n",
    "def calculate_metrics(true, preds):\n",
    "    preds_class = (preds > 0.5).astype(int)\n",
    "    return {\n",
    "        'auc': roc_auc_score(true, preds),\n",
    "        'pr_auc': average_precision_score(true, preds),\n",
    "        'f1': f1_score(true, preds_class),\n",
    "        'accuracy': accuracy_score(true, preds_class),\n",
    "        'precision': precision_score(true, preds_class),\n",
    "        'recall': recall_score(true, preds_class),\n",
    "        'confusion_matrix': confusion_matrix(true, preds_class)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "40e1bdacf409ca5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T21:49:58.453652Z",
     "start_time": "2025-09-03T21:49:58.448933Z"
    }
   },
   "outputs": [],
   "source": [
    "# 5. Função de Avaliação\n",
    "@track_emissions()\n",
    "def evaluate_model(model, loader, criterion):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels = labels.float().unsqueeze(1).to(DEVICE)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            preds = torch.sigmoid(outputs).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    metrics = calculate_metrics(np.array(all_labels), np.array(all_preds))\n",
    "    return np.mean(losses), metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b238c8f45b58ef58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T21:50:00.364077Z",
     "start_time": "2025-09-03T21:50:00.357965Z"
    }
   },
   "outputs": [],
   "source": [
    "# 4. Função de Treinamento com Early Stopping\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, fold):\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    patience = 3\n",
    "\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'auc': [],\n",
    "        'f1': []\n",
    "    }\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels = labels.float().unsqueeze(1).to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        # Validação\n",
    "        val_loss, val_metrics = evaluate_model(model, val_loader, criterion)\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "        # Registrar métricas\n",
    "        history['train_loss'].append(epoch_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['auc'].append(val_metrics['auc'])\n",
    "        history['f1'].append(val_metrics['f1'])\n",
    "\n",
    "        print(f'Fold {fold}, Epoch {epoch+1}/{EPOCHS}')\n",
    "        print(f'Train Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f}')\n",
    "        print(f'Val AUC: {val_metrics[\"auc\"]:.4f}, F1: {val_metrics[\"f1\"]:.4f}')\n",
    "\n",
    "        # Early Stopping\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), f'best_model_fold{fold}.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping at epoch {epoch+1}')\n",
    "                break\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2fbc1216bd17783b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T21:50:03.479203Z",
     "start_time": "2025-09-03T21:50:03.475403Z"
    }
   },
   "outputs": [],
   "source": [
    "# 7. Grad-CAM\n",
    "def generate_grad_cam(model, img_tensor, target_layer):\n",
    "    cam = GradCAM(model=model, target_layers=[target_layer], use_cuda=torch.cuda.is_available())\n",
    "    grayscale_cam = cam(input_tensor=img_tensor.unsqueeze(0))\n",
    "    grayscale_cam = grayscale_cam[0, :]\n",
    "    visualization = show_cam_on_image(img_tensor.permute(1,2,0).cpu().numpy(), grayscale_cam, use_rgb=True)\n",
    "    return visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "37f4fc2f62f5328b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T21:50:05.094841Z",
     "start_time": "2025-09-03T21:50:05.087456Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_training_histories(histories):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    # Cores para diferentes folds\n",
    "    colors = ['b', 'g', 'r', 'c', 'm']\n",
    "\n",
    "    # Plot Loss\n",
    "    plt.subplot(2, 2, 1)\n",
    "    for i, history in enumerate(histories):\n",
    "        epochs = range(1, len(history['train_loss'])+1)\n",
    "        plt.plot(epochs, history['train_loss'], linestyle='--', color=colors[i], label=f'Fold {i+1} Train')\n",
    "        plt.plot(epochs, history['val_loss'], linestyle='-', color=colors[i], label=f'Fold {i+1} Val')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "    # Plot AUC\n",
    "    plt.subplot(2, 2, 2)\n",
    "    for i, history in enumerate(histories):\n",
    "        epochs = range(1, len(history['auc'])+1)\n",
    "        plt.plot(epochs, history['auc'], color=colors[i], label=f'Fold {i+1}')\n",
    "    plt.title('Validation AUC')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('AUC')\n",
    "    plt.legend()\n",
    "    plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "    # Plot F1-Score\n",
    "    plt.subplot(2, 2, 3)\n",
    "    for i, history in enumerate(histories):\n",
    "        epochs = range(1, len(history['f1'])+1)\n",
    "        plt.plot(epochs, history['f1'], color=colors[i], label=f'Fold {i+1}')\n",
    "    plt.title('Validation F1-Score')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.legend()\n",
    "    plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_histories.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2d02f635f732115b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T21:50:07.774104Z",
     "start_time": "2025-09-03T21:50:07.765836Z"
    }
   },
   "outputs": [],
   "source": [
    "# 8. Validação Cruzada\n",
    "@track_emissions()\n",
    "def cross_validation():\n",
    "\n",
    "    dataset = DFUDataset(\"../data/ischaemia\", transform=transform)\n",
    "    skf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)  # Alterado para StratifiedGroupKFold\n",
    "\n",
    "    all_metrics = []\n",
    "    fold_histories = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(\n",
    "        X=np.zeros(len(dataset.labels)),  # Dummy data\n",
    "        y=dataset.labels,\n",
    "        groups=dataset.identifiers  # Grupos dos identificadores\n",
    "    )):\n",
    "        print(f'\\nFold {fold+1}/5')\n",
    "\n",
    "        # Split adicional: 80% treino, 20% teste interno\n",
    "        train_idx, test_idx = train_test_split(\n",
    "            train_idx,\n",
    "            test_size=0.2,\n",
    "            stratify=np.array(dataset.labels)[train_idx],\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        # Split datasets\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "        test_subsampler = torch.utils.data.SubsetRandomSampler(test_idx)\n",
    "        val_subsampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
    "\n",
    "        train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=train_subsampler)\n",
    "        test_loader = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=test_subsampler)\n",
    "        val_loader = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=val_subsampler)\n",
    "\n",
    "        # Inicialização\n",
    "        model = create_model()\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-4)  # L2 regularization\n",
    "\n",
    "        history = train_model(model, train_loader, val_loader, criterion, optimizer, fold+1)\n",
    "        fold_histories.append(history)\n",
    "\n",
    "        plot_training_histories(fold_histories)\n",
    "\n",
    "        # Avaliação no conjunto de teste interno\n",
    "        model.load_state_dict(torch.load(f'best_model_fold{fold+1}.pth'))\n",
    "        _, test_metrics = evaluate_model(model, test_loader, criterion)\n",
    "\n",
    "        # Carregar melhor modelo e avaliar\n",
    "        model.load_state_dict(torch.load(f'best_model_fold{fold+1}.pth'))\n",
    "        _, metrics = evaluate_model(model, val_loader, criterion)\n",
    "\n",
    "         # Adicione as métricas de teste\n",
    "        all_metrics.append({\n",
    "            'fold': fold+1,\n",
    "            'val_metrics': metrics,\n",
    "            'test_metrics': test_metrics\n",
    "        })\n",
    "\n",
    "    pd.DataFrame(fold_histories).to_csv('results/train_08/training_history.csv', index=False)\n",
    "\n",
    "\n",
    "    # Processar all_metrics para formato tabular\n",
    "    metrics_rows = []\n",
    "    for m in all_metrics:\n",
    "        row = {\n",
    "            'fold': m['fold'],\n",
    "            **{f'val_{k}': v for k, v in m['val_metrics'].items()},\n",
    "            **{f'test_{k}': v for k, v in m['test_metrics'].items()}\n",
    "        }\n",
    "        metrics_rows.append(row)\n",
    "\n",
    "    pd.DataFrame(metrics_rows).to_csv('results/train_08/fold_metrics.csv', index=False)\n",
    "\n",
    "    # Relatório Final\n",
    "    print(\"\\nResultados da Validação Cruzada:\")\n",
    "\n",
    "    # Para métricas de validação\n",
    "    print(\"\\n--- Métricas de Validação ---\")\n",
    "    for key in all_metrics[0]['val_metrics']:\n",
    "        if key != 'confusion_matrix':\n",
    "            mean_val = np.mean([m['val_metrics'][key] for m in all_metrics])\n",
    "            std_val = np.std([m['val_metrics'][key] for m in all_metrics])\n",
    "            print(f'Val_{key.upper()}: {mean_val:.4f} ± {std_val:.4f}')\n",
    "\n",
    "    # Para métricas de teste interno\n",
    "    print(\"\\n--- Métricas de Teste Interno ---\")\n",
    "    for key in all_metrics[0]['test_metrics']:\n",
    "        if key != 'confusion_matrix':\n",
    "            mean_test = np.mean([m['test_metrics'][key] for m in all_metrics])\n",
    "            std_test = np.std([m['test_metrics'][key] for m in all_metrics])\n",
    "            print(f'Test_{key.upper()}: {mean_test:.4f} ± {std_test:.4f}')\n",
    "\n",
    "    return fold_histories, all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88383674328c2c64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T10:39:37.362466Z",
     "start_time": "2025-09-03T21:50:11.208503Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Executar\n",
    "if __name__ == \"__main__\":\n",
    "    fold_histories, all_metrics = cross_validation()\n",
    "\n",
    "    # Exemplo Grad-CAM\n",
    "    # model = create_model().eval()\n",
    "    # sample_img, _ = dataset[0]\n",
    "    # target_layer = model.layer4[-1] if MODEL_NAME == \"resnet50\" else model.features[-1]\n",
    "    # cam_image = generate_grad_cam(model, sample_img.to(DEVICE), target_layer)\n",
    "    #\n",
    "    # plt.imshow(cam_image)\n",
    "    # plt.title('Grad-CAM Visualization')\n",
    "    # plt.axis('off')\n",
    "    # plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
