{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T11:01:59.193047Z",
     "start_time": "2025-09-04T11:01:47.471210Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models import ResNet50_Weights, EfficientNet_B0_Weights\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import (roc_auc_score, average_precision_score,\n",
    "                             f1_score, accuracy_score, precision_score,\n",
    "                             recall_score, confusion_matrix)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "\n",
    "from codecarbon import track_emissions"
   ],
   "id": "5768b4999c7f9058",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T11:02:31.681532Z",
     "start_time": "2025-09-04T11:02:31.672827Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Configurações iniciais\n",
    "TASK = \"Ischaemia\"  # \"Infection\" ou \"Ischaemia\"\n",
    "IMG_SIZE = 256\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "MODEL_NAME = \"resnet50\"  # \"efficientnet\" ou \"resnet50\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "id": "1a51056c6c5c9d42",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T11:02:45.621713Z",
     "start_time": "2025-09-04T11:02:45.610156Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Pré-processamento de Imagens\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ],
   "id": "364763197381a8b1",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T11:02:59.644737Z",
     "start_time": "2025-09-04T11:02:59.627593Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2. Dataset Customizado\n",
    "class DFUDataset(Dataset):\n",
    "    def __init__(self, root_dir, task=TASK, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.task = task\n",
    "\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.identifiers = []  # Novo campo para identificadores únicos\n",
    "\n",
    "        # Validate that root_dir is a valid path\n",
    "        if not isinstance(root_dir, str):\n",
    "            raise TypeError(f\"Expected root_dir to be a string, but got {type(root_dir).__name__}\")\n",
    "\n",
    "        print(\"TASK: \" + TASK)\n",
    "\n",
    "        positive_dir = os.path.join(root_dir, task, \"Aug-Positive\")\n",
    "        negative_dir = os.path.join(root_dir, task, \"Aug-Negative\")\n",
    "\n",
    "\n",
    "        # Carregar imagens positivas\n",
    "        for img_name in os.listdir(positive_dir):\n",
    "            self.image_paths.append(os.path.join(positive_dir, img_name))\n",
    "            self.labels.append(1)\n",
    "            # Extrair identificador único (ex: \"imagem\" de \"imagem_1X_M.jpg\")\n",
    "            identifier = img_name.split('_')[0]\n",
    "            self.identifiers.append(identifier)\n",
    "\n",
    "        # Carregar imagens negativas\n",
    "        for img_name in os.listdir(negative_dir):\n",
    "            self.image_paths.append(os.path.join(negative_dir, img_name))\n",
    "            self.labels.append(0)\n",
    "            identifier = img_name.split('_')[0]\n",
    "            self.identifiers.append(identifier)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label, self.identifiers[idx]"
   ],
   "id": "2ff57b27d73bcb7d",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T11:03:12.208637Z",
     "start_time": "2025-09-04T11:03:12.199752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3. Modelo com Regularização\n",
    "def create_model():\n",
    "    if MODEL_NAME == \"resnet50\":\n",
    "        weights = ResNet50_Weights.IMAGENET1K_V1  # Ou ResNet50_Weights.DEFAULT\n",
    "        model = models.resnet50(weights=weights)\n",
    "        num_ftrs = model.fc.in_features\n",
    "        model.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(num_ftrs, 1)\n",
    "        )\n",
    "    elif MODEL_NAME == \"efficientnet\":\n",
    "        weights = EfficientNet_B0_Weights.IMAGENET1K_V1  # Ou EfficientNet_B0_Weights.DEFAULT\n",
    "        model = models.efficientnet_b0(weights=weights)\n",
    "        num_ftrs = model.classifier[1].in_features\n",
    "        model.classifier[1] = nn.Sequential(\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(num_ftrs, 1)\n",
    "        )\n",
    "    return model.to(DEVICE)"
   ],
   "id": "a7b185a97d7a8e1f",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T11:03:37.380879Z",
     "start_time": "2025-09-04T11:03:37.370391Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 6. Cálculo de Métricas\n",
    "def calculate_metrics(true, preds):\n",
    "    preds_class = (preds > 0.5).astype(int)\n",
    "    return {\n",
    "        'auc': roc_auc_score(true, preds),\n",
    "        'pr_auc': average_precision_score(true, preds),\n",
    "        'f1': f1_score(true, preds_class),\n",
    "        'accuracy': accuracy_score(true, preds_class),\n",
    "        'precision': precision_score(true, preds_class),\n",
    "        'recall': recall_score(true, preds_class),\n",
    "        'confusion_matrix': confusion_matrix(true, preds_class)\n",
    "    }"
   ],
   "id": "a7a435ef1a6a1064",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T11:04:03.616949Z",
     "start_time": "2025-09-04T11:04:03.607909Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 5. Função de Avaliação\n",
    "@track_emissions()\n",
    "def evaluate_model(model, loader, criterion):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, _ in loader:\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels = labels.float().unsqueeze(1).to(DEVICE)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            preds = torch.sigmoid(outputs).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    metrics = calculate_metrics(np.array(all_labels), np.array(all_preds))\n",
    "    return np.mean(losses), metrics"
   ],
   "id": "690c1c5db61e3c90",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T11:04:13.423077Z",
     "start_time": "2025-09-04T11:04:13.402259Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 4. Função de Treinamento com Early Stopping\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, fold):\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    patience = 10 # Aumentar a paciência pode ser útil\n",
    "\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'auc': [],\n",
    "        'f1': []\n",
    "    }\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for inputs, labels, _ in train_loader:\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels = labels.float().unsqueeze(1).to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        # Validação\n",
    "        val_loss, val_metrics = evaluate_model(model, val_loader, criterion)\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "        # Registrar métricas\n",
    "        history['train_loss'].append(epoch_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['auc'].append(val_metrics['auc'])\n",
    "        history['f1'].append(val_metrics['f1'])\n",
    "\n",
    "        print(f'Fold {fold}, Epoch {epoch+1}/{EPOCHS}')\n",
    "        print(f'Train Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f}')\n",
    "        print(f'Val AUC: {val_metrics[\"auc\"]:.4f}, F1: {val_metrics[\"f1\"]:.4f}')\n",
    "\n",
    "        # Early Stopping\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), f'best_model_fold{fold}.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping at epoch {epoch+1}')\n",
    "                break\n",
    "\n",
    "    return history"
   ],
   "id": "a6b71bc29bd90e75",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T11:04:34.563505Z",
     "start_time": "2025-09-04T11:04:34.552295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 7. Grad-CAM\n",
    "def generate_grad_cam(model, img_tensor, target_layer):\n",
    "    cam = GradCAM(model=model, target_layers=[target_layer], use_cuda=torch.cuda.is_available())\n",
    "    grayscale_cam = cam(input_tensor=img_tensor.unsqueeze(0))\n",
    "    grayscale_cam = grayscale_cam[0, :]\n",
    "    visualization = show_cam_on_image(img_tensor.permute(1,2,0).cpu().numpy(), grayscale_cam, use_rgb=True)\n",
    "    return visualization"
   ],
   "id": "a673fe04ac6abfcd",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T11:04:47.798206Z",
     "start_time": "2025-09-04T11:04:47.777479Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_training_histories(histories):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    # Cores para diferentes folds\n",
    "    colors = ['b', 'g', 'r', 'c', 'm']\n",
    "\n",
    "    # Plot Loss\n",
    "    plt.subplot(2, 2, 1)\n",
    "    for i, history in enumerate(histories):\n",
    "        epochs = range(1, len(history['train_loss'])+1)\n",
    "        plt.plot(epochs, history['train_loss'], linestyle='--', color=colors[i], label=f'Fold {i+1} Train')\n",
    "        plt.plot(epochs, history['val_loss'], linestyle='-', color=colors[i], label=f'Fold {i+1} Val')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "    # Plot AUC\n",
    "    plt.subplot(2, 2, 2)\n",
    "    for i, history in enumerate(histories):\n",
    "        epochs = range(1, len(history['auc'])+1)\n",
    "        plt.plot(epochs, history['auc'], color=colors[i], label=f'Fold {i+1}')\n",
    "    plt.title('Validation AUC')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('AUC')\n",
    "    plt.legend()\n",
    "    plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "    # Plot F1-Score\n",
    "    plt.subplot(2, 2, 3)\n",
    "    for i, history in enumerate(histories):\n",
    "        epochs = range(1, len(history['f1'])+1)\n",
    "        plt.plot(epochs, history['f1'], color=colors[i], label=f'Fold {i+1}')\n",
    "    plt.title('Validation F1-Score')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.legend()\n",
    "    plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_histories.png')\n",
    "    plt.show()"
   ],
   "id": "948e004523708dc7",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T11:05:58.978808Z",
     "start_time": "2025-09-04T11:05:58.939176Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 8. Validação Cruzada (CORRIGIDO)\n",
    "@track_emissions()\n",
    "def cross_validation_corrected():\n",
    "\n",
    "    # Carrega o dataset completo\n",
    "    full_dataset = DFUDataset(\"../data/ischaemia\", transform=transform)\n",
    "\n",
    "    # Converte para numpy para facilitar a manipulação\n",
    "    labels = np.array(full_dataset.labels)\n",
    "    identifiers = np.array(full_dataset.identifiers)\n",
    "\n",
    "    # Validação Cruzada Estratificada por Grupo\n",
    "    skf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    all_metrics = []\n",
    "    fold_histories = []\n",
    "\n",
    "    # O split agora é feito sobre os índices do dataset completo\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(labels)), labels, identifiers)):\n",
    "        print(f'Fold {fold+1}/5')\n",
    "\n",
    "        # Cria os datasets de treino e validação para o fold atual\n",
    "        train_dataset = Subset(full_dataset, train_idx)\n",
    "        val_dataset = Subset(full_dataset, val_idx)\n",
    "\n",
    "        # Cria os DataLoaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        print(f\"Tamanho do treino: {len(train_dataset)} imagens\")\n",
    "        print(f\"Tamanho da validação: {len(val_dataset)} imagens\")\n",
    "\n",
    "        # Inicialização do modelo, critério e otimizador\n",
    "        model = create_model()\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-4)  # L2 regularization\n",
    "\n",
    "        # Treina o modelo\n",
    "        history = train_model(model, train_loader, val_loader, criterion, optimizer, fold+1)\n",
    "        fold_histories.append(history)\n",
    "\n",
    "        # Plota os gráficos de treinamento após cada fold\n",
    "        plot_training_histories(fold_histories)\n",
    "\n",
    "        # Carrega o melhor modelo salvo pelo Early Stopping e avalia no conjunto de validação\n",
    "        model.load_state_dict(torch.load(f'best_model_fold{fold+1}.pth'))\n",
    "        _, metrics = evaluate_model(model, val_loader, criterion)\n",
    "\n",
    "        print(f\"Métricas finais do Fold {fold+1} (no conjunto de validação):\")\n",
    "        for key, value in metrics.items():\n",
    "            if key != 'confusion_matrix':\n",
    "                print(f'\t{key}: {value:.4f}')\n",
    "            else:\n",
    "                print(f'\t{key}:{value}')\n",
    "\n",
    "        all_metrics.append(metrics)\n",
    "\n",
    "    return fold_histories, all_metrics"
   ],
   "id": "831b034c88d7b05a",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Executar a validação cruzada corrigida\n",
    "if __name__ == \"__main__\":\n",
    "    fold_histories, all_metrics = cross_validation_corrected()\n",
    "\n",
    "    # Calcular e imprimir a média das métricas entre os folds\n",
    "    avg_metrics = {key: np.mean([m[key] for m in all_metrics if key != 'confusion_matrix']) for key in all_metrics[0] if key != 'confusion_matrix'}\n",
    "    print(\"--- Média das Métricas nos 5 Folds ---\")\n",
    "    for key, value in avg_metrics.items():\n",
    "        print(f'\t{key}: {value:.4f}')\n",
    "\n",
    "    # Somar as matrizes de confusão\n",
    "    total_cm = np.sum([m['confusion_matrix'] for m in all_metrics], axis=0)\n",
    "    print(\"Matriz de Confusão Total:\")\n",
    "    print(total_cm)"
   ],
   "id": "4e1a879d9c61cd37"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
